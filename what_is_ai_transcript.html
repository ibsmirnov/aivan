<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What is AI transcript</title>
    <link rel="stylesheet" href="assets/style.css">
</head>
<body>
    <nav class="site-nav"><a href="/">← Return Home</a>  <a href="about_the_author.html">About</a></nav>
    <article>
        <h1>What is AI transcript</h1>
<p>This is a transcript for 
<a href="https://youtu.be/RpZ4RGQNwt8">What is AI video</a> cleaned by AI.</p>
<p>Hello, my name is Ivan. Welcome to “Getting Started with GenAI in Research.”</p>
<p>By now, you‘ve likely either tried or at least heard about the new chatbots powered by large language models that perform remarkably well across a wide variety of tasks. For instance, you can now ask a computer to generate a rhyming proof that there is an infinite number of prime numbers, and it will dutifully oblige. While we can debate the quality of this output, not long ago, the idea of accomplishing something like this would have seemed unimaginable.</p>
<p>In this course, we will discuss:
1) How this technology (called generative AI because it generates text, images, or videos) works
2) The best ways to interact with GenAIchatbots 3)
How it can help us in research 4)
Importantly, why we should be careful when using these technologies</p>
<p>We’ll start by exploring how this technology works. My goal is to provide you with a high-level understanding of the technologies behind GenAI. Strictly speaking, you don‘t need this—you could directly jump into the chatbot interface and start interacting with it. However, I believe this knowledge is valuable for two reasons:</p>
<p>1) Curiosity has always been important for researchers
2) Understanding how these models work will make you a better user of the technology and help you critically evaluate statements about them, whether overly negative or positive</p>
<p>The goal for this part is to demystify all the acronyms you might encounter nowadays—AI, ML, NN, GPT, NLP, and so on—and help you understand what stands behind them at a high level.</p>
<p>Let’s start with AI, or artificial intelligence. There isn‘t a universally accepted definition of artificial intelligence, but typically it means using computers to solve tasks that would otherwise require human intelligence. Interestingly, once humans learn how to automate a certain task, they quickly stop considering it “smart enough” to qualify as artificial intelligence.</p>
<p>For example, no one considers a calculator AI because it seems to be such a trivial task—you’re just computing numbers. But imagine the time when the first mechanical calculator was created. These calculators couldn‘t even outperform humans, yet they seemed remarkable. Some people would have considered modern electronic calculators impossible—magical technology—and might have called them artificial intelligence because they outperform humans in complex computations.</p>
<p>A more recent example is chess. I personally remember when a computer first beat the human world champion—it was shocking and made headlines everywhere. The Economist wrote something like “if your job is similar to chess, prepare to lose it.” Everyone thought this event would have massive implications. But nowadays, I have a chess engine on my mobile phone that could easily outperform any human, and we don’t consider it remarkable or “real AI.” Similarly, I believe that in the future, we might not consider today‘s large language models and chatbots “smart enough” to be AI once we get used to the fact that these tasks can be easily automated.</p>
<p>Let me discuss one specific AI task that I find particularly interesting: image recognition. What makes this task fascinating is that unlike chess, which requires specialised knowledge and skill, image recognition is something humans do effortlessly. For example, if my son sees a couple of images of cats, he can easily recognise other cats, even if I poorly draw something that merely suggests a cat. Yet if I ask him what’s 2+3, he wouldn‘t be able to answer because he’s just two years old. For him, maths is very complicated, but he can effortlessly solve this recognition task that computers couldn‘t manage until recently.
The reason this is complicated for computers is that it’s hard to develop an algorithm for classification. We can‘t rely on:</p>
<p>1) Colour, because it can vary so much
2) Shape, which can also be vastly different
3) Body parts—if we define a cat as something with a tail, we’d fail to recognise tailless cats
4) Features like nose and eyes alone wouldn‘t be sufficient to identify a cat in abstract art</p>
<p>For humans, it’s straightforward—you can immediately see that all these images are cats. People have been trying to solve this problem since the early 1960s, when the field of artificial intelligence emerged. They started with simple tasks, using photos of basic objects, trying to identify contours and from these contours determine an object‘s position and direction. They created increasingly complicated models and achieved some progress.</p>
<p>To measure this progress, in 2010 a competition called ImageNet was created. In this competition, you have photorealistic images and must guess what’s in them. You get five guesses, and if at least one is correct, you‘ve successfully recognised the image. In 2010, the performance of the best model was slightly below 30%, and it was improving slowly as people built upon previous algorithms.</p>
<p>Then something significant happened in 2012. The performance dramatically improved, and computers quickly outperformed humans. Interestingly, the ideas behind the 2012 algorithm weren’t built upon previous approaches—it was something entirely new. Instead of defining a cat by certain texture, shape, etc., the new approach simply recognised a cat as something similar to other images of cats and dissimilar to images of dogs, horses, or birds.
This idea might seem simple, making you wonder why it wasn‘t used before. The reason was twofold:</p>
<p>1) There wasn’t enough data. Since the 1980s, the amount of digital data started to rival analog data, growing exponentially. What we had in 2007 is nothing compared to 2021. Without all this digital photography, all these images of cats, this approach couldn‘t work.
2) It required substantial computational power, which was steadily growing according to Moore’s Law.</p>
<p>This approach is called machine learning. Instead of writing a deterministic algorithm that performs certain operations and produces an output like “this is a dog,” you create an algorithm that relies on data—training data. You train your algorithm on this data, and once trained, you can apply it to new, unseen datasets.</p>
<p>In fact, all recent advances in artificial intelligence have been driven by machine learning, which we’ll explore in more detail in our next video.</p>
    </article>
    <footer class="site-footer"><p>Copyright &copy; 2025 University of Technology Sydney</p></footer>
</body>
</html>